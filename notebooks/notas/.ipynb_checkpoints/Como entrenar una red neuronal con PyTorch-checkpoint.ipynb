{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Como entrenar una red neuronal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenar una red neuronal es un proceso iterativo; en cada iteración (se suelen llamar *epoch*) el modelo hace una suposición sobre los valores de salida, calcula el error (*loss*), realiza las derivadas del error respecto a sus parametros y optimiza los parametros utlizando el descenso por gradiente. En este [video](https://www.youtube.com/watch?v=tIeHLnjs5U8) se explica en forma detallada el proceso de retropropagación o *backpropagation*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.6%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "119.3%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparámetros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los hyperparámetros son parámetros ajustables que permiten controlar el proceso de optimización del modelo. Diferentes valores de hyperparámetros impactan en el entrenamiento del modelo y en la tasa de convergencia.\n",
    "\n",
    "Definimos los siguintes hyperparámetros para el entrenamiento:\n",
    "* **Número de Epochs** - es el número de iteraciones sobre el dataset.\n",
    "* **Batch Size** - el número de muestras que se propagan a través de la red antes de que los parámetros sean actualizados.\n",
    "* **Learning Rate** - indica cuanto actualizar los parámetros en cada batch/epoch. Valores pequeños hacen que el entrenamiento sea más lento, y valores grandes pueden llevar a resultados impredecibles durante el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop de optimización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez que tenemos seteados nuestros hyperparámetros, podemos entrenar y optimizar el modelo con un loop de optimización que se llama **epoch**.\n",
    "\n",
    "Cada epoch consiste de dos partes:\n",
    "* **El Loop de Entrenamiento** - consiste en iterar sobre el dataset e intentar converger a parámetros optimos.\n",
    "* **El Loop de Testeo o Validación** - consiste en iterar sobre el dataset de test para confirmar que el modelo este mejorando."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función de perdida o error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando introducimos datos de entrenamiento en nuestra red no entrenada es probable que la red no acierte los valores correctos. **La función de perdida** mide la diferencia entre el resultado obtenido y el resultado esperado, y es esta función la que deseamos minimizar durante el entrenamiento. \n",
    "\n",
    "Para calcular este error hacemos predicciones utilizando los datos de entrada y los comparamos con los datos de salida que reales que ya conocemos.\n",
    "\n",
    "Entre las funciones de perdida comunes se incluyen ```nn.MSELoss``` (Mean Square Error) para tareas de regresión, ```nn.NLLLoss``` (Negative Log Likelihood) para clasificación y ```nn.CrossEntropyLoss``` que es la combinación de ```nn.LogSoftmax``` y ```nn.NLLLoss```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La optimización es el proceso de ajuste de los parametros del modelo de manera de reducir el error en cada paso de entrenamiento. Los **Algortimos de Optimización** definen como se realiza este proceso. La lógica de optimización esta encapsulada dentro del objeto ```optimizer```. En este notebook utilizamos Stochastic Gradient Descent, pero existen diferentes optimizadores disponibles en PyTorch como ADAM y RMSProp.\n",
    "Inicializamos el modelo registrando los parámetros del modelo y pasando el hyperparámetro **learning rate**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dentro del loop de entrenamiento, la optimización se produce en tres pasos:\n",
    "* Se ejecuta la función ```optimizer.zero_grad()``` para resetear los gradientes de los parámetros del modelo. Los gradientes siempre se suman, para prevenir el doble conteo entre iteraciones, se ponen a cero.\n",
    "* Se retropropaga el error de predicción ejecutando ```loss.backwards()```.\n",
    "* Una vez que se obtienen los gradientes, se ejecuta ```optimizer.step()``` para ajustar los parámetros en la retropropagación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementación completa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos ```train_loop``` que itera sobre el código de optimización, y ```test_loop``` que evalua el desempeño del modelo en comparación con los datos de testeo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicializamos el función de optimización y le pasamos ```train_loop``` y ```test_loop```. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.300666  [    0/60000]\n",
      "loss: 2.299901  [ 6400/60000]\n",
      "loss: 2.288499  [12800/60000]\n",
      "loss: 2.286347  [19200/60000]\n",
      "loss: 2.293461  [25600/60000]\n",
      "loss: 2.258206  [32000/60000]\n",
      "loss: 2.273653  [38400/60000]\n",
      "loss: 2.259931  [44800/60000]\n",
      "loss: 2.256156  [51200/60000]\n",
      "loss: 2.222188  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 24.6%, Avg loss: 2.250172 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.248859  [    0/60000]\n",
      "loss: 2.275531  [ 6400/60000]\n",
      "loss: 2.248182  [12800/60000]\n",
      "loss: 2.248298  [19200/60000]\n",
      "loss: 2.266625  [25600/60000]\n",
      "loss: 2.196623  [32000/60000]\n",
      "loss: 2.225963  [38400/60000]\n",
      "loss: 2.207388  [44800/60000]\n",
      "loss: 2.198159  [51200/60000]\n",
      "loss: 2.133656  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 26.1%, Avg loss: 2.194909 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 2.193150  [    0/60000]\n",
      "loss: 2.242248  [ 6400/60000]\n",
      "loss: 2.197535  [12800/60000]\n",
      "loss: 2.196558  [19200/60000]\n",
      "loss: 2.223935  [25600/60000]\n",
      "loss: 2.124814  [32000/60000]\n",
      "loss: 2.159803  [38400/60000]\n",
      "loss: 2.142185  [44800/60000]\n",
      "loss: 2.123550  [51200/60000]\n",
      "loss: 2.021828  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 25.9%, Avg loss: 2.126772 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 2.122305  [    0/60000]\n",
      "loss: 2.197370  [ 6400/60000]\n",
      "loss: 2.136384  [12800/60000]\n",
      "loss: 2.132723  [19200/60000]\n",
      "loss: 2.168204  [25600/60000]\n",
      "loss: 2.049030  [32000/60000]\n",
      "loss: 2.079682  [38400/60000]\n",
      "loss: 2.069458  [44800/60000]\n",
      "loss: 2.038464  [51200/60000]\n",
      "loss: 1.903320  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 27.0%, Avg loss: 2.053928 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 2.042769  [    0/60000]\n",
      "loss: 2.141410  [ 6400/60000]\n",
      "loss: 2.056892  [12800/60000]\n",
      "loss: 2.057558  [19200/60000]\n",
      "loss: 2.107258  [25600/60000]\n",
      "loss: 1.955817  [32000/60000]\n",
      "loss: 1.992604  [38400/60000]\n",
      "loss: 1.975400  [44800/60000]\n",
      "loss: 1.914133  [51200/60000]\n",
      "loss: 1.801891  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 35.4%, Avg loss: 1.965718 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.914468  [    0/60000]\n",
      "loss: 2.047848  [ 6400/60000]\n",
      "loss: 1.938125  [12800/60000]\n",
      "loss: 1.977333  [19200/60000]\n",
      "loss: 2.049867  [25600/60000]\n",
      "loss: 1.861742  [32000/60000]\n",
      "loss: 1.916491  [38400/60000]\n",
      "loss: 1.888173  [44800/60000]\n",
      "loss: 1.812612  [51200/60000]\n",
      "loss: 1.727512  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 37.1%, Avg loss: 1.894324 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.806643  [    0/60000]\n",
      "loss: 1.968757  [ 6400/60000]\n",
      "loss: 1.835673  [12800/60000]\n",
      "loss: 1.914755  [19200/60000]\n",
      "loss: 2.001878  [25600/60000]\n",
      "loss: 1.785817  [32000/60000]\n",
      "loss: 1.859538  [38400/60000]\n",
      "loss: 1.820315  [44800/60000]\n",
      "loss: 1.741141  [51200/60000]\n",
      "loss: 1.675562  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 37.5%, Avg loss: 1.840107 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 1.724783  [    0/60000]\n",
      "loss: 1.908463  [ 6400/60000]\n",
      "loss: 1.756492  [12800/60000]\n",
      "loss: 1.867712  [19200/60000]\n",
      "loss: 1.963555  [25600/60000]\n",
      "loss: 1.726785  [32000/60000]\n",
      "loss: 1.817977  [38400/60000]\n",
      "loss: 1.769501  [44800/60000]\n",
      "loss: 1.692114  [51200/60000]\n",
      "loss: 1.639178  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 38.0%, Avg loss: 1.798975 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 1.664279  [    0/60000]\n",
      "loss: 1.863404  [ 6400/60000]\n",
      "loss: 1.696533  [12800/60000]\n",
      "loss: 1.830849  [19200/60000]\n",
      "loss: 1.931650  [25600/60000]\n",
      "loss: 1.680060  [32000/60000]\n",
      "loss: 1.785323  [38400/60000]\n",
      "loss: 1.729589  [44800/60000]\n",
      "loss: 1.653467  [51200/60000]\n",
      "loss: 1.611802  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 38.4%, Avg loss: 1.765480 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 1.614916  [    0/60000]\n",
      "loss: 1.829087  [ 6400/60000]\n",
      "loss: 1.649651  [12800/60000]\n",
      "loss: 1.800214  [19200/60000]\n",
      "loss: 1.901898  [25600/60000]\n",
      "loss: 1.641086  [32000/60000]\n",
      "loss: 1.753786  [38400/60000]\n",
      "loss: 1.696787  [44800/60000]\n",
      "loss: 1.621372  [51200/60000]\n",
      "loss: 1.576143  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 44.9%, Avg loss: 1.695606 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
